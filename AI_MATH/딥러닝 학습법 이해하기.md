<div align='center'>
  <h1> 비선형 모델(Nueral Network) <h1/>
</div>
  
## 신경망을 수식으로 분해하면?
<img src="https://user-images.githubusercontent.com/57162812/150110016-65b8b07e-dea9-4e23-a3b8-e8f670b7acbf.png" width=400>
  
<img src="https://latex.codecogs.com/svg.image?d" />차원의 <img src="https://latex.codecogs.com/svg.image?\overrightarrow{x}" />에서 <img src="https://latex.codecogs.com/svg.image?p" />차원의 <img src="https://latex.codecogs.com/svg.image?\overrightarrow{o}" />로의 선형 모델을 생각해보자. 즉, <img src="https://latex.codecogs.com/svg.image?d" />개의 변수, <img src="https://latex.codecogs.com/svg.image?p" />개의 선형 모델로 <img src="https://latex.codecogs.com/svg.image?p" />개의 잠재변수를 설명하는 모델이 된다. 따라서 아래의 그림과 같이 <img src="https://latex.codecogs.com/svg.image?dp" />개의 화살표가 필요로한다.
  
![image](https://user-images.githubusercontent.com/57162812/150111895-25188e55-2220-4dbc-922b-5f8751fc52ed.png)

즉, <img src="https://latex.codecogs.com/svg.image?d" /><img src="https://latex.codecogs.com/svg.image?\times" /><img src="https://latex.codecogs.com/svg.image?p" /> 차원의 가중치 벡터가 필요로 하게 된다. 이것이 가중치 행렬이다.
  
## softmax 연산
  분류 문제의 경우, 선형모델을 통해 나온 결과벡터를 확률로 해석할 수 있는 확률 벡터로 변환시켜준다. 분류 문제의 모델에서의 신경망은 <img src="https://latex.codecogs.com/svg.image?softmax(\overrightarrow{o})=softmax(W\overrightarrow{x}+\overrightarrow{b})" />로 선형 모델과 소프트 맥스의 합성 함수라고 할 수 있다.
  
  소프트 맥스 함수의 식은 다음과 같다.
  
  <img src="https://latex.codecogs.com/svg.image?softmax(\overrightarrow{x})=(\frac{\exp(o_1)}{\sum_{i=1}^{p}\exp(o_i)},...,\frac{\exp(o_p)}{\sum_{i=1}^{p}\exp(o_i)})" />
  
  ```python
  def softmax(x):
    denumerator=np.exp(x-np.max(x, axis=-1, keepdims=True)) 
    # max를 빼주는 이유는? 
    # 지수함수를 사용하는 만큼 값이 크면 함수값은 기하급수적으로 늘어난다. 따라서 효율성을 위해 최댓값을 빼주면서 softmax 함수를 구현할 수 있다.
    numerator=np.sum(denumerator, axis=-1, keepdims=True)
    return denumerator/numerator
  ```
  
  - 추론의 경우에는 최댓값을 가진 주소만 1로 출력하는 one=hot vector를 사용하면 되므로 softmax 연산을 사용하지 않는다.
  
## 신경망 : 선형모델과 활성화 함수를 합성한 함수
  <img src="https://latex.codecogs.com/svg.image?\sigma" /> : 활성화 함수  
  <img src="https://latex.codecogs.com/svg.image?\sigma(\overrightarrow{z})=\sigma(W\overrightarrow{x}+\overrightarrow{b})" />  
  <img src="https://latex.codecogs.com/svg.image?\overrightarrow{z}" />를 우리는 잠재벡터(hidden vector) 또는 뉴련이라고 한다.  
  <img src="https://latex.codecogs.com/svg.image?H=(\sigma(\overrightarrow{z}_1),...,\sigma(\overrightarrow{z}_n))" />  
  
  ![image](https://user-images.githubusercontent.com/57162812/150115933-e34610e7-166e-4789-ba7b-e4b26f7aa27f.png)

### 활성화 함수 : 상수에서 상수로의 함수로 비선형 함수이다.
  이전에는 sigmoid함수와 tanh 함수를 많이 사용하였지만, 요즘에는 ReLU 함수를 주로 사용한다.  
  <img src="https://latex.codecogs.com/svg.image?sigmoid(x)=\frac{1}{1+\exp(-x)}" />  
  <img src="https://latex.codecogs.com/svg.image?tanh(x)=\frac{\exp(x)-\exp(-x)}{\exp(x)+\exp(-x)}" />  
  <img src="https://latex.codecogs.com/svg.image?ReLU(x)=max\{0,x\}" />
  
### 2층(2-layers) 신경망 : 선형 변환을 2번 실행 
  <img src="https://latex.codecogs.com/svg.image?\overrightarrow{x}\rightarrow{W}^{(1)}\overrightarrow{x}+\overrightarrow{b}=\overrightarrow{z}\rightarrow\sigma(\overrightarrow{z})=\sigma({W}^{(1)}\overrightarrow{x}+\overrightarrow{b})" />  
  <img src="https://latex.codecogs.com/svg.image?H=(\sigma(\overrightarrow{z}_1,...,\overrightarrow{z}_n))=(\overrightarrow(h_1),...,\overrightarrow(h_n))" />  
  
  ∴ <img src="https://latex.codecogs.com/svg.image?\overrightarrow{o}={W}^{(2)}\overrightarrow{h}+{\overrightarrow{b}}^{(2)}" />
### 다층(multi-layers) 신경망(Perceptron) : MLP
  선형변환을 반복한 신경망
  - 층을 여러번 쌓는 이유는? 층이 깊을수록 목적함수를 근사하는데에 필요한 뉴런(노드)의 개수가 빨리 줄어들어 효율적으로 복잡한 모델을 만들 수 있다.
##딥러닝의 학습 원리 : 역전파
  각 층에서 사용된 parameter 가중치 행렬 <img src="https://latex.codecogs.com/svg.image?W" />과 가중치 벡터 <img src="https://latex.codecogs.com/svg.image?\overrightarrow{b}" />를 학습한다. 즉, 윗층에서부터 역순으로 Chain Rule을 기반으로한 자동미분을 통해 gradient vector를 계산한다.
