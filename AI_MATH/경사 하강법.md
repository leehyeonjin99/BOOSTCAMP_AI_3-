# 미분 : 변수의 움직임에 따른 함수값의 변화를 측정
- 최적화에서 많이 사용  
- 공식 : <img src="https://user-images.githubusercontent.com/57162812/149910788-2cc5f57a-0487-47e7-b196-7388435a2596.png" width=150>

```python
import sympy as sym
from sympy.abc import x

sym.diff(sym.poly(x**2+2*x+1), x)
```

- f'(a)는 (a,f(a))에서의 접선의 기울기이다.
- 기울기를 알게 되면 x값을 어느 방향으로 움직여야 함수값이 증가/감소하는지 알 수 있다.
  - 미분값을 더해주면 함수값 증가 : 경사 상승법, 최대값
  - 미분값을 빼주면 함수값 감소 : 경사 하강법, 최솟값
  - <img src="https://user-images.githubusercontent.com/57162812/149915246-3195b2d8-8ede-4dc6-a1d8-35eec5c28ca8.png" width=150>
  - 극값에 도달하게 되면 미분값이 0이 되므로 움직임이 중단된다. : 극소값에 갖히게 될 수 있다.

```python
import sympy as sym
from sympy.abc import x
import numpy as np

def func(fun, val): # 함수값과 함수를 반환하는 함수
  fun=sym.poly(fun)
  return fun.subs(x, val), fun

def gradient(fun, val): # 미분값과 미분을 반환하는 함수
  _,function=func(fun, val)
  diff=sym.diff(function, x)
  return diff.subs(x, val), diff
  
def gradient_descent(fun, init_x, lr=1e-2, epsilon=1e-5): # 경사 하강법 실행
  cnt=0
  val=init_x
  diff, _=gradient(fun, init_x) 
  while np.abs(diff)>epsilon: # 종료 조건을 설정해준다 : gradient가 0에 가까워질 때까지
    val=val-lr*diff
    diff, _ =gradient(fun, val)
    cnt+=1
  print("함수 : {}, 연산회숫 : {}, 최소점 : ({}, {})".format(func(fun, val)[1], cnt, val, func(fun, val)[0]))
```

```python
gradient_descent(x**2+2*x+3, np.random.uniform(-2, 2))
```
![image](https://user-images.githubusercontent.com/57162812/149920785-663823cc-3cb2-4bce-8ff0-fde18ab053ab.png)

## 정의역 변수가 벡터인 경우 : 편미분(Partial Differetiation)
<img src="https://user-images.githubusercontent.com/57162812/149921482-4c0bf9ad-88ed-4af6-96c6-18469585313d.png" width=200>

ex)  
<img src="https://user-images.githubusercontent.com/57162812/149921808-08275b0b-e310-4c4f-83bd-54d9f770163e.png" width=300>

- 각 변수별로 편미분을 계산한 그레디언트 벡터로 경사 하강/경사 상승법 사용 가능하다.
- <img src="https://user-images.githubusercontent.com/57162812/149922236-14d2c2de-cf0b-4210-95f0-f08a382746a9.png" width=200>
  - ∇f : 가장 빨리 증가하는 방향 (그래프 위에 ∇f의 방향으로 나타내면 최댓값을 향한다.)
  - -∇f : 가장 빨리 감소하는 방향 (그래프 위에 -∇f의 방향으로 나타내면 최솟값을 향한다.)
- 코드 구현시 epsilon과의 크기를 비교할 때, np.abs을 사용하던 것을 norm을 사용하여 비교해주면 된다.

## 경사 하강법으로 선형 회귀 계수 구하기
- 목적식 : <img src="https://user-images.githubusercontent.com/57162812/149923050-2173076c-5b95-4921-9f96-86470192800b.png" width=80> : L2-norm
  - 목적식을 최소화하는 <img src="https://user-images.githubusercontent.com/57162812/149923245-dda1f34f-44bf-45a1-a219-1432a1d036c1.png" width=10>를 찾아야 한다. : <img src="https://user-images.githubusercontent.com/57162812/149923245-dda1f34f-44bf-45a1-a219-1432a1d036c1.png" width=10>에 대하여 목적식을 미분한다.  
    <img src="https://user-images.githubusercontent.com/57162812/149923909-a2bb4072-8423-4246-906b-a4c08bdf6661.png" width=300>  
    <img src="https://user-images.githubusercontent.com/57162812/149924488-ebb143c1-4c8e-4f7a-b854-5de6b557d5b5.png" width=400>
- 목적식 : <img src="https://user-images.githubusercontent.com/57162812/149924699-51ca344f-f312-4e93-bee9-5f7cfc71706c.png" width=80>   
  <img src="https://user-images.githubusercontent.com/57162812/149925116-3f346547-4bf2-4542-8d49-84d4ed2d9935.png" width=400>

```python
for t in range(T):
  error=y-X@beta
  grad=-np.transpose(X)@error
  beta=beta-lr*grad
```

- 학습률과 학습횟수는 중요한 hyperparameter가 된다.
- 목적식이 convex인 경우 적절한 T와 lr에 대해서는 최솟값으로 수렴을 보장한다. 하지만, 목적식이 convex가 아닌 경우에는 수렴을 보장할 수 없다.
-
## 경사 하강법의 한게 극복 방법 : 확률적 경사 하강법(SGD)
- 데이터를 한개 혹은 일부(미니배치)를 활용하여 업데이트한다. 따라서, 연산 자원을 좀 더 효율적으로 활용하는데에 도움이 된다.즉, 경사 하강법의 하드웨어 상의 한계를 극복 가능하다.

### SGD의 원리 : 미니배치 연산
- 경사하강법은 전체 데이터 D=(X, y)에 대하여 gradeint vector를 연산
- SGD는 전체 데이터 D의 부분집합인 미니배치를 가지고 gradient vector 연산 : 미니 배치를 확률적으로 선택하므로 목적식의 모양이 바뀐다. : 극소값이 탈출 가능하다. : convex 함수가 아니더라도 최솟값 수렴을 보장할 수 있다.
