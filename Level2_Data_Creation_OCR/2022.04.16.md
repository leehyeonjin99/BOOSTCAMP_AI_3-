# âœ”ï¸ì‹¤í—˜
### ì‹¤í—˜ ëª©í‘œ

- OCR taskì— ì•Œë§ëŠ” *Optimizer* ë° *Learning rate*ì— ëŒ€í•´ì„œ ì°¾ì„ ìˆë‹¤.

### ë™ê¸°

- í•™ìŠµ ì„±ëŠ¥ì— ìˆì–´ì„œ ê°€ì¥ í° êµ¬ì„± ìš”ì†ŒëŠ” `Model` + `Data` + `Optimizer` +  `Loss Function` + `Scheduler` + `Augmentation` ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.
- ì´ ì¤‘ ì˜¤ëŠ˜ì€ ìµœì ì˜ **Optimizer**ë¥¼ ì°¾ê³ , í•´ë‹¹ Optimizerì— ì í•©í•œ **Learning Rate**ë¥¼ ì°¾ì•„ë³´ê³ ì í•œë‹¤.

### í™˜ê²½ ë° ê³¼ì •

- optimizerë¥¼ ì„¤ì •í•˜ê¸° ìœ„í•´ì„œ `torch`, `adamp`, `madgrad` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.
  - `torch 1.7.1` : *SGD, ASGD, Adam, AdamW, Adamax*
  - `adamp` : *ADAMP*
  - `madgrad` : *MADGRAD*
- optimizer ì‹¤í—˜ì„ ì™„ë£Œí•œ í›„ ê°€ì¥ ì ì€ Mean Lossë¥´ ê°–ëŠ” optimizerë¥¼ ì„ ì •í•œë‹¤.
- í•´ë‹¹ optimizerì— ëŒ€í•˜ì—¬ ë‹¤ì–‘í•œ Learning Rateë¥¼ ì ìš©í•˜ì—¬ ê°€ì¥ ë‚®ì€ Mean Lossë¥´ ê°–ëŠ Learning Rateë¥¼ ì°¾ì•„ ì•ìœ¼ë¡œì˜ ì‹¤í—˜ì—ì„œ ê³ ì •í•œë‹¤.


> ğŸ“ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ
> - `Mean Loss` = `epoch_loss` / `num_batches`
>   - `epoch_loss` : ê° epochë§ˆë‹¤ ëª¨ë“  batchì˜ lossë¥¼ í•©í•œ ê°’

> ğŸ“ optimizer ì ìš© ë°©ë²•
> ```python
> from importlib import import_module
> import madgrad
> import adamp
> 
> if optimizer in dir(torch.optim):
>     opt_module = getattr(import_module('torch.optim'), optimizer)
>     optimizer = opt_module(model.parameters(), lr=learning_rate)
> elif optimizer == 'MADGRAD':
>     optimizer = madgrad.MADGRAD(model.parameters(), lr=learning_rate)
> elif optimizer == 'AdamP':
>     optimizer = adamp.AdamP(model.parameters(), lr=learning_rate)
> ```

### ê²°ê³¼

<img width="880" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-04-17 á„‹á…©á„’á…® 9 24 56" src="https://user-images.githubusercontent.com/57162812/163714171-c930fed3-3b0c-4f96-998b-727cfbaafb5c.png">

# âœ”ï¸í”¼ë“œë°±
### ì•Œê²Œëœ ì 
- í•´ë‹¹ OCR taskì—ì„œëŠ” Adam ê³„ì—´ì—ì„œ ì„±ëŠ¥ì´ ì¢‹ê²Œ ë‚˜ì™”ë‹¤.
- Adam ê³„ì—´ê³¼ SGD ê³„ì—´ì„ ë¹„êµí•´ ë³´ì•˜ì„ ë•Œ, ì‹¤í—˜ ë‚´ìš© ì¤‘ SGDëŠ” 0.1 ê·¸ë¦¬ê³  Adamì€ 0.001ì—ì„œ ìµœì ì˜ ê°’ì„ ê°€ì¡‹ë‹¤.

### ì•„ì‰¬ìš´ ì 
- CUDAì™€ pytorchì˜ ë²„ì „ í˜¸í™˜ ë¬¸ì œë¡œ pytorch versionì„ 1.7.1ì—ì„œ 1.11.0ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ í•˜ì§€ ëª»í•˜ì˜€ë‹¤. ë”°ë¼ì„œ, NAdam, RAdamì„ ì‹¤í—˜í•´ë³´ì§€ ëª»í•œ ì ì´ ì•„ì‰½ë‹¤. ë¬¼ë¡ , CUDAì˜ ë²„ì „ì„ ë°”ê¿”ì¤„ ìˆ˜ë„ ìˆì—ˆê² ì§€ë§Œ, ì•ì—ì„œì˜ ì‹¤í—˜ê³¼ ì¼ê´€ì„±ì„ ìœ„í•´ì„œ ìœ ì§€í•˜ì˜€ë‹¤.  
  > CUDA 11.0ì€ torch 1.7.1ê³¼ í˜¸í™˜ ê°€ëŠ¥í•˜ë‹¤.
  [previous pytorch version](https://pytorch.org/get-started/previous-versions/)
  
### ì•ìœ¼ë¡œì˜ ë°©í–¥
- ìµœì ì˜ `Optimizer` + `Learning Rate`ì˜ ìµœì ì˜ ì¡°í•©ì„ ì°¾ì€ í›„ ê³ ì •í•˜ì—¬ ê°€ì Learning Rateë¥¼ ì¡°ì ˆí•˜ëŠ” `Scheduler`ì— ëŒ€í•´ì„œ ì‹¤í—˜í•œë‹¤.
- torchë¥¼ upgradeê°€ ê°€ëŠ¥í•˜ë„ë¡ CUDAë¥¼ ì„¤ì •í•´ë³¸ë‹¤.






