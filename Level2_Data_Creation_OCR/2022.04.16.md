# ✔️실험
### 실험 목표

- OCR task에 알맞는 *Optimizer* 및 *Learning rate*에 대해서 찾을 있다.

### 동기

- 학습 성능에 있어서 가장 큰 구성 요소는 `Model` + `Data` + `Optimizer` +  `Loss Function` + `Scheduler` + `Augmentation` 로 볼 수 있다.
- 이 중 오늘은 최적의 **Optimizer**를 찾고, 해당 Optimizer에 적합한 **Learning Rate**를 찾아보고자 한다.

### 환경 및 과정

- optimizer를 설정하기 위해서 `torch`, `adamp`, `madgrad` 라이브러리를 사용하였다.
  - `torch 1.7.1` : *SGD, ASGD, Adam, AdamW, Adamax*
  - `adamp` : *ADAMP*
  - `madgrad` : *MADGRAD*
- optimizer 실험을 완료한 후 가장 적은 Mean Loss르 갖는 optimizer를 선정한다.
- 해당 optimizer에 대하여 다양한 Learning Rate를 적용하여 가장 낮은 Mean Loss르 갖느 Learning Rate를 찾아 앞으로의 실험에서 고정한다.


> 📎 성능 평가 지표
> - `Mean Loss` = `epoch_loss` / `num_batches`
>   - `epoch_loss` : 각 epoch마다 모든 batch의 loss를 합한 값

> 📎 optimizer 적용 방법
> ```python
> from importlib import import_module
> import madgrad
> import adamp
> 
> if optimizer in dir(torch.optim):
>     opt_module = getattr(import_module('torch.optim'), optimizer)
>     optimizer = opt_module(model.parameters(), lr=learning_rate)
> elif optimizer == 'MADGRAD':
>     optimizer = madgrad.MADGRAD(model.parameters(), lr=learning_rate)
> elif optimizer == 'AdamP':
>     optimizer = adamp.AdamP(model.parameters(), lr=learning_rate)
> ```

### 결과

<img width="880" alt="스크린샷 2022-04-17 오후 9 24 56" src="https://user-images.githubusercontent.com/57162812/163714171-c930fed3-3b0c-4f96-998b-727cfbaafb5c.png">

# ✔️피드백
### 알게된 점
- 해당 OCR task에서는 Adam 계열에서 성능이 좋게 나왔다.
- Adam 계열과 SGD 계열을 비교해 보았을 때, 실험 내용 중 SGD는 0.1 그리고 Adam은 0.001에서 최적의 값을 가졋다.

### 아쉬운 점
- CUDA와 pytorch의 버전 호환 문제로 pytorch version을 1.7.1에서 1.11.0으로 업그레이드 하지 못하였다. 따라서, NAdam, RAdam을 실험해보지 못한 점이 아쉽다. 물론, CUDA의 버전을 바꿔줄 수도 있었겠지만, 앞에서의 실험과 일관성을 위해서 유지하였다.  
  > CUDA 11.0은 torch 1.7.1과 호환 가능하다.
  [previous pytorch version](https://pytorch.org/get-started/previous-versions/)
  
### 앞으로의 방향
- 최적의 `Optimizer` + `Learning Rate`의 최적의 조합을 찾은 후 고정하여 가자 Learning Rate를 조절하는 `Scheduler`에 대해서 실험한다.
- torch를 upgrade가 가능하도록 CUDA를 설정해본다.






