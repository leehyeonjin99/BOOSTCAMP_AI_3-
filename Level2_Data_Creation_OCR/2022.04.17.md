# ✔️실험
### 실험 목표

- 앞선 실험에서 얻은 최적의 `optimizer` + `loss` 조합을 사용하여 가장 적합한 `Scheduler`를 찾을 수 있다.
- 가장 최적의 augmentation 조합을 알아낼 수 있다. with 진혁 캠퍼님

### 동기

- learning rate의 중요성
  - learning rate가 계속해서 작은 값을 유지한다면, 속도가 너무 느리며 local minimum에 빠질 수 있다.
  - 반면, learning rate가 계속해서 작은 값을 유지한다면, 한번에 큰 이동으로 최솟값에 도달하지 못할 수 있다.
  - 따라서, learning rate를 잘 조정해주는 scheduler가 필요로 하다.
- augmentaiton의 중요성.  
  - [참고](https://arxiv.org/pdf/2108.06949.pdf)


### 과정

- `torch.optim.lr_scheduler` 모듈에 존재하는  scheduler를 사용하였다.
  - MultiStepLR
  - MultiplicativeLR
  - CosineAnnealingLR
  - ReduceLROnPlateau

### 결과
<img width="938" alt="스크린샷 2022-04-18 오전 2 06 48" src="https://user-images.githubusercontent.com/57162812/163724993-e3e20cf4-13fb-4b8d-ace9-84bb014b0671.png">

# ✔️피드백
### 알게된 점
### 아쉬운 점
### 앞으로의 방향
