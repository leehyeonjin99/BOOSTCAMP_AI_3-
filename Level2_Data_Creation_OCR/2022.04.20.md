# ✔️실험
### 실험 목표

- ICDAR2019 + ICDAR ArT에 대해서 더 나은 성과를 얻을 수 있다.
- optimizer, scheduler 수정

### 동기

- 대회 기간까지 얼마 남지 않은 것을 고려해 epoch를 줄이고 이에 맞는 learning rate scheduler 설정

### 과정

- Dataset : ICDAR2019 + ICDAR ArT
- max epoch : 150
  - 대회 직전 종료 시점을 고려하여 200 → 150
- optimizer : `MADGRAD` with start learning rate 0.001
  - optimizer 실험 결과에 따른 선택
- scheduler : `MultiStepLR` with milestones = [50, 100]
  - learning rate의 지속적인 유지로 인해 최적의 값으로 찾아가지 못하는 듯 하여 [max epoch//2] → [max epoch//3, max epoch//3\*2]


### 결과

|epoch|mean loss|test f1|test recall|test recall|
|:-:|:-:|:-:|:-:|:-:|
|40|0.8057|0.6160|0.5237|0.7477|
|65|0.6347|0.6897|0.6076|0.7974|

# ✔️피드백
### 알게된 점

- optimizer 변경의 효과는 있었다. optimizer와 learning rate가 같은 시점에서 Adam과 MADGRAD를 비교해본 결과 0.6095와 0.6160으로 MADGRAD 사용시 조금 더 높은 것을 알 수 있었다.

### 아쉬운 점

- 해당 실험은 150 epoch까지 측정하지 못할 것 같아 아쉽다.


### 앞으로의 방향
- polygon annotation 수정 방법 정확히 이해하기
- validation deteval 방법 정확히 이해하기
- 내일이 마지막 대회날이다. 마지막까지 최선으 다하자!!🔥🔥🔥🔥

