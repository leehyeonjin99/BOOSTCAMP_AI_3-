### 2022.02.07 MON
#### 학습 목표/완료 상황
- [x]  DL Basic 3강까지 + 학습 정리 + 퀴즈 + 과제
- [x]  질문 정리하기
- [ ]  코테 1문제
- [ ]  Kaggle Notebook  1개 따라해보기

#### 피어세션

---

### 2022.02.09 WED
#### 학습 목표/완료 상황
- [x]  9-10강 듣기
- [x]  6-8강 내용정리
- [x]  면접 스터디 정리
- [x]  심화과제1
- [ ]  심화과제2
- [x]  코테 1문제

#### 피어세션
- 면접 질문 스터디 : 발표자 이현진
  - Cross Validation
    - 장점 : 모든 데이터를 학습에 사용하여 일반화 성능이 좋아지며, 모든 데이터를 검증에 사용하여 과적합이 방지된다.
    - 단점 : iteration 수가 많아 훈련 및 평가 시간이 소요된다.
    - Hold-Out CV, K-Fold CV, LOOCV, LpOCV
  - XGBoost
    - 앙상블의 부스팅 기법
    - GBM(Gradient Boosting Machine)을 기반으로 하였으며, 단점들을 보완
    - Kaggle에서 우수한 성적을 내고 있어 인기 있다.
  - 앙상블
    - 부스팅 : 병렬적 진행 : 오버 피팅 문제 모델에 적합
    - 배깅 : 순차적 진행 : 성능 문제 모델에 적합
    - 스태킹 : 개별 기반 모델 + 최종 메타 모델

- 논문 스터디 및 CV Algorithm 스터디 대회 시작 전까지 매일 진행 결정

#### 과제 리뷰
- einpose라는 library를 알게 되었다. 차원 관리에 있어서 매우 용이하다.

```python
from einops import rearrange, reduce, repeat
# rearrange elements according to the pattern
output_tensor = rearrange(input_tensor, 't b c -> b c t')
# combine rearrangement and reduction
output_tensor = reduce(input_tensor, 'b c (h h2) (w w2) -> b h w c', 'mean', h2=2, w2=2)
# copy along a new axis 
output_tensor = repeat(input_tensor, 'h w -> h w c', c=3)
```
---

### 2022.02.10 THU
#### 학습 목표/완료 상황
- [x]  Data VIz 1-3강+학습정리
- [x]  DL Basic 9-10강 정리
- [x]  심화과제 2
- [x]  코테 1문제
- [ ]  resnet 정리

#### 피어세션
- 면접 질문 스터디
  - Feature Vector
    - Data의 각 특징들을 모아놓은 Vector
  - 좋은 모델이란?
    - 고려사항 : 정확성, 속도, 크기
   - 작은 의사결정나무가 큰 의사결정나무보다 괜찮은가?
    - 의사결정나무는 모델의 크기가 커질수록 과대적합이되는 경우가 있다.
    - 해결책 : 가지치기

  - 논문 스터디 : EfficientNet
    - 컴퓨터 자원의 한계, 여유에 맞춰 network의 depth, width, resolution을 모두 scale up 하는 방법에 대한 정량적인 해법 및 새로운 network 제시
    - Depth scale up : generalization performance 향상, capture richer, complex feature / vanishing gradient
    - Width scale up : 보통 작은 network에서 사용 / fine-grained feature를 더 잘 뽑아내고 train이 쉽다. / 너무 넓고 얕은 network는 higher level feature를 뽑아내기 어렵다.
    - Resolution scale up : potential fine-grained patternㅇ르 잘 뽑아낸다

#### 오피스 아워
- ViT
- AAE

직접 작성해보며 구현해볼 필요가 있을 것 같다.

---

### 2022.02.11 FRI
#### 학습 목표/완료 상황
- [x]  심화 과제 1,2 따라 해보기
- [x]  LSTM 모델 따라 해보기
- [ ]  ResNet 논문 정리
- [ ]  SqueezeNet 논문 읽기 시작
- [x]  코팅 테스트 1문제

#### 피어세션
- 면접 질문 스터디
  - 스팸필터로 Logistic Regression 사용 이유는?
    - Logistic Regression : 데이터가 어떤 범주에 속할 확률을 0과 1 사이의 값으로 예측하고 그 확률에 따라 더 높은 가능성의 범주로 분류하는 지도 학습 기법
    - S자 모형의 함수가 필요하며, 0과 1 사이의 값으로 표현해야하기 때문에 Logistic Regression을 사용한다.
  - OLS Regression 공식
    - 오차의 제곱을 최소화하는 w와 b의 값을 찾는 방식
  - 딥러닝
    - 인간의 신경망을 모방한 심층신경망 이론을 기반으로 고안된 머신러닝 방식의 일종으로 수많은 패턴 속에서 스스로 패턴을 학습한다.
    - 머신러닝은 컴퓨터에게 먼저 다양한 정보를 가르치고 그 학습한 결과에 따라 컴퓨터가 새로운 것을 예측 딥러닝은 머신러닝의 심화된 기술로서 인간의 가르침이라는 과정을 거치지 않아도 스스로 학습하고 미래의 상황을 예측한다.

#### 멘토링

- OCT 데이터를 통한 안구 질환 분류 : 정상/고도 난시/그 외의 질병 : 좌우 반전에 대해 mirroring 전처리 사용
- 음성 데이터를 통한 파킨슨 질병 분류 : 음성 데이터 전처리 : 단위별로 나누어 특정값 이상이면 copy, 아니면 pass
- Toy project 및 augmentation 기법들 적용해보기
