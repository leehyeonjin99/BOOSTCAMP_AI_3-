### 2022.02.07 MON
#### 학습 목표/완료 상황
- [x]  DL Basic 3강까지 + 학습 정리 + 퀴즈 + 과제
- [x]  질문 정리하기
- [ ]  코테 1문제
- [ ]  Kaggle Notebook  1개 따라해보기

#### 피어세션

---

### 2022.02.09 WED
#### 학습 목표/완료 상황
- [x]  9-10강 듣기
- [x]  6-8강 내용정리
- [x]  면접 스터디 정리
- [x]  심화과제1
- [ ]  심화과제2
- [x]  코테 1문제

#### 피어세션
- 면접 질문 스터디 : 발표자 이현진
  - Cross Validation
    - 장점 : 모든 데이터를 학습에 사용하여 일반화 성능이 좋아지며, 모든 데이터를 검증에 사용하여 과적합이 방지된다.
    - 단점 : iteration 수가 많아 훈련 및 평가 시간이 소요된다.
    - Hold-Out CV, K-Fold CV, LOOCV, LpOCV
  - XGBoost
    - 앙상블의 부스팅 기법
    - GBM(Gradient Boosting Machine)을 기반으로 하였으며, 단점들을 보완
    - Kaggle에서 우수한 성적을 내고 있어 인기 있다.
  - 앙상블
    - 부스팅 : 병렬적 진행 : 오버 피팅 문제 모델에 적합
    - 배깅 : 순차적 진행 : 성능 문제 모델에 적합
    - 스태킹 : 개별 기반 모델 + 최종 메타 모델

- 논문 스터디 및 CV Algorithm 스터디 대회 시작 전까지 매일 진행 결정

#### 과제 리뷰
- einpose라는 library를 알게 되었다. 차원 관리에 있어서 매우 용이하다.

```python
from einops import rearrange, reduce, repeat
# rearrange elements according to the pattern
output_tensor = rearrange(input_tensor, 't b c -> b c t')
# combine rearrangement and reduction
output_tensor = reduce(input_tensor, 'b c (h h2) (w w2) -> b h w c', 'mean', h2=2, w2=2)
# copy along a new axis 
output_tensor = repeat(input_tensor, 'h w -> h w c', c=3)
```

### 2022.02.10 THU
#### 학습 목표/완료 상황
- [x]  Data VIz 1-3강+학습정리
- [x]  DL Basic 9-10강 정리
- [x]  심화과제 2
- [x]  코테 1문제
- [ ]  resnet 정리

#### 피어세션
- 면접 질문 스터디
  - Feature Vector
    - Data의 각 특징들을 모아놓은 Vector
  - 좋은 모델이란?
    - 고려사항 : 정확성, 속도, 크기
   - 작은 의사결정나무가 큰 의사결정나무보다 괜찮은가?
    - 의사결정나무는 모델의 크기가 커질수록 과대적합이되는 경우가 있다.
    - 해결책 : 가지치기

  - 논문 스터디 : EfficientNet
    - 컴퓨터 자원의 한계, 여유에 맞춰 network의 depth, width, resolution을 모두 scale up 하는 방법에 대한 정량적인 해법 및 새로운 network 제시
    - Depth scale up : generalization performance 향상, capture richer, complex feature / vanishing gradient
    - Width scale up : 보통 작은 network에서 사용 / fine-grained feature를 더 잘 뽑아내고 train이 쉽다. / 너무 넓고 얕은 network는 higher level feature를 뽑아내기 어렵다.
    - Resolution scale up : potential fine-grained patternㅇ르 잘 뽑아낸다

#### 오피스 아워
- ViT
- AAE

직접 작성해보며 구현해볼 필요가 있을 것 같다.
